{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a baseline model using a Language Model (LM) such as a Bigram or Trigram model in Python with TensorFlow, you'll need to use the TensorFlow Datasets library for loading the data, and the TensorFlow text and numpy libraries for preprocessing and constructing the model. Here's an example of creating a Bigram model as a baseline model using TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "# Define the input function for loading and preprocessing the data\n",
    "def load_and_preprocess():\n",
    "    # Load the data as a TensorFlow dataset\n",
    "    data, info = tfds.load('tfds:mldatasets/protein/aa_structure_seqs', with_info=True, shuffle_files=False, as_supervised=True)\n",
    "    # Filter the data to extract the relevant features and labels\n",
    "    sequences = data['sequence']\n",
    "    labels = tf.constant([[label] for label in data['class']])\n",
    "    # Convert the sequences to lowercase and tokenize them\n",
    "    sequences = tf.strings.map(lambda x: tf.strings.lower(x), sequences)\n",
    "    sequences = tf.strings.split(sequences, b\" \")\n",
    "    sequences = tf.stack(sequences, axis=1)\n",
    "    sequences = tf.cast(sequences, tf.int32)\n",
    "    # Create a dictionary of unique tokens and their indices\n",
    "    vocab_size = len(tf.unique(sequences).numpy())\n",
    "    vocab = {token: i for i, token in enumerate(tf.unique(sequences).numpy())}\n",
    "    return sequences, labels, vocab, vocab_size\n",
    "\n",
    "# Load and preprocess the data\n",
    "sequences, labels, vocab, vocab_size = load_and_preprocess()\n",
    "\n",
    "# Create the training, validation, and test datasets\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((sequences[:int(0.8 * len(sequences))], labels[:int(0.8 * len(labels))])).batch(batch_size)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((sequences[int(0.8 * len(sequences)):], labels[int(0.8 * len(labels)):])).batch(batch_size)\n",
    "test_dataset = sequences[int(0.95 * len(sequences)):]\n",
    "\n",
    "# Create the Bigram model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=5, input_length=None),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(128, return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(128)),\n",
    "    tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "model.compile(loss='sparse_categorical_crossent', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Create the Bigram model baseline\n",
    "baseline_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=5, input_length=2),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64, return_sequences=False)),\n",
    "    tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "baseline_model.compile(loss='sparse_categorical_crossent', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the models and evaluate their performance\n",
    "history = model.fit(train_dataset, validation_data=val_dataset, epochs=epochs)\n",
    "baseline_history = baseline_model.fit(train_dataset, validation_data=val_dataset, epochs=epochs)\n",
    "\n",
    "# Print the performance of the models\n",
    "print(\"Model accuracy: {:.2f}\".format(history.history['val_accuracy'][-1]))\n",
    "print(\"Baseline model accuracy: {:.2f}\".format(baseline_history.history['val_accuracy'][-1]))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
